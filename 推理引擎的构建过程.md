# 推理引擎的构建过程
## 创建logger
用于捕捉构建过程中的提示和错误信息
```c++
TRTLogger glogger_;
```
## 创建builder
需要在builder中创建network和engine，builder还将存储配置文件，比如是否启用DLA，设置最大工作空间等
```c++
auto builder = TRTUniquePtr<nvinfer1::IBuilder>(nvinfer1::createInferBuilder(glogger_));
```
## 创建network
此时的network是个空壳子，没有填充权重
```c++
TRTUniquePtr<nvinfer1::INetworkDefinition> network;
if(mParams.model_type == ModelType::Onnx)
{
   const auto explicitBatch = 1U << static_cast<uint32_t>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
   network = TRTUniquePtr<nvinfer1::INetworkDefinition>(builder->createNetworkV2(explicitBatch));
}
else
{
   network = TRTUniquePtr<nvinfer1::INetworkDefinition>(builder->createNetworkV2(0));
}
```
## 创建解析器
可以创建caffe，onnx，uff解析器，用于解析对应格式的神经网络权重文件，并将其填充到network
```c++
auto parser = TRTUniquePtr<nvcaffeparser1::ICaffeParser>(nvcaffeparser1::createCaffeParser());
const nvcaffeparser1::IBlobNameToTensor* blobNameToTensor = parser->parse(locateFile(mParams.prototxtFileName, 
                   mParams.data_dirs).c_str(),
                   locateFile(mParams.file_path, mParams.data_dirs).c_str(), *network, DataType::kFLOAT);
```
## 创建推理engine
从builder中创建cudaengine
```c++
mEngine = std::shared_ptr<nvinfer1::ICudaEngine>(builder->buildCudaEngine(*network), samplesCommon::InferDeleter());
```
# 执行
## 实例化engine推理所需要的缓存

## 从engine中实例化推理上下文context

## 处理输入，将数据读取到buffer缓存

## 执行前向传播

## 处理输出，顾名思义
